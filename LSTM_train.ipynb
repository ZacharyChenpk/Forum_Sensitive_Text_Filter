{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pkuseg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../holes_github_dict.pkl','rb') as f:\n",
    "    holes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "857186 10796\n"
     ]
    }
   ],
   "source": [
    "with open('../pids_github.pkl','rb') as f:\n",
    "    pos_pids = pickle.load(f)\n",
    "with open('../deleted_pids.pkl','rb') as f:\n",
    "    neg_pids = pickle.load(f)\n",
    "    \n",
    "pol_pids = list(set(pos_pids) - set(neg_pids))\n",
    "print(len(pos_pids), len(neg_pids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport word2vec\\nimport matplotlib\\nimport matplotlib.pyplot as plt\\nfrom sklearn.decomposition import PCA\\nword2vec.word2vec('corpusSegDone.txt', 'corpusWord2Vec.bin', size=300,verbose=True)\\nmodel = word2vec.load('corpusWord2Vec.bin')\\nrawWordVec=model.vectors\\n\\n# reduce the dimension of word vector\\nX_reduced = PCA(n_components=2).fit_transform(rawWordVec)\\n\\n# show some word(center word) and it's similar words\\nindex1,metrics1 = model.similar(u'中国')\\nindex2,metrics2 = model.similar(u'清华')\\nindex3,metrics3 = model.similar(u'牛顿')\\nindex4,metrics4 = model.similar(u'自动化')\\nindex5,metrics5 = model.similar(u'刘亦菲')\\n\\n# add the index of center word \\nindex01=np.where(model.vocab==u'中国')\\nindex02=np.where(model.vocab==u'清华')\\nindex03=np.where(model.vocab==u'牛顿')\\nindex04=np.where(model.vocab==u'自动化')\\nindex05=np.where(model.vocab==u'刘亦菲')\\n\\nindex1=np.append(index1,index01)\\nindex2=np.append(index2,index03)\\nindex3=np.append(index3,index03)\\nindex4=np.append(index4,index04)\\nindex5=np.append(index5,index05)\\n\\n# plot the result\\n#zhfont = matplotlib.font_manager.FontProperties(fname='/usr/share/fonts/truetype/wqy/wqy-microhei.ttc')\\nfig = plt.figure()\\nax = fig.add_subplot(111)\\n\\nfor i in index1:\\n    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='r')\\n\\nfor i in index2:\\n    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='b')\\n\\nfor i in index3:\\n    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='g')\\n\\nfor i in index4:\\n    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='k')\\n\\nfor i in index5:\\n    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='c')\\n\\nax.axis([0,0.8,-0.5,0.5])\\nplt.show()\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import word2vec\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "word2vec.word2vec('corpusSegDone.txt', 'corpusWord2Vec.bin', size=300,verbose=True)\n",
    "model = word2vec.load('corpusWord2Vec.bin')\n",
    "rawWordVec=model.vectors\n",
    "\n",
    "# reduce the dimension of word vector\n",
    "X_reduced = PCA(n_components=2).fit_transform(rawWordVec)\n",
    "\n",
    "# show some word(center word) and it's similar words\n",
    "index1,metrics1 = model.similar(u'中国')\n",
    "index2,metrics2 = model.similar(u'清华')\n",
    "index3,metrics3 = model.similar(u'牛顿')\n",
    "index4,metrics4 = model.similar(u'自动化')\n",
    "index5,metrics5 = model.similar(u'刘亦菲')\n",
    "\n",
    "# add the index of center word \n",
    "index01=np.where(model.vocab==u'中国')\n",
    "index02=np.where(model.vocab==u'清华')\n",
    "index03=np.where(model.vocab==u'牛顿')\n",
    "index04=np.where(model.vocab==u'自动化')\n",
    "index05=np.where(model.vocab==u'刘亦菲')\n",
    "\n",
    "index1=np.append(index1,index01)\n",
    "index2=np.append(index2,index03)\n",
    "index3=np.append(index3,index03)\n",
    "index4=np.append(index4,index04)\n",
    "index5=np.append(index5,index05)\n",
    "\n",
    "# plot the result\n",
    "#zhfont = matplotlib.font_manager.FontProperties(fname='/usr/share/fonts/truetype/wqy/wqy-microhei.ttc')\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "for i in index1:\n",
    "    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='r')\n",
    "\n",
    "for i in index2:\n",
    "    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='b')\n",
    "\n",
    "for i in index3:\n",
    "    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='g')\n",
    "\n",
    "for i in index4:\n",
    "    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='k')\n",
    "\n",
    "for i in index5:\n",
    "    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='c')\n",
    "\n",
    "ax.axis([0,0.8,-0.5,0.5])\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/czb/anaconda3/envs/torchsparse/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.04781811 -0.04908484 -0.00073652 -0.01611761 -0.08490036 -0.05371191\n",
      " -0.01265323 -0.0452581  -0.08618768 -0.0927384 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import word2vec\n",
    "model = word2vec.load('corpusWord2Vec.bin')\n",
    "a='保研'\n",
    "print(model[a][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from utils import split_train, hole_dzdataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_pos_pids, test_pos_pids = split_train(np.array(pos_pids), 0.2)\n",
    "train_neg_pids, test_neg_pids = split_train(np.array(neg_pids), 0.2)\n",
    "train_neg_pids = np.array(list(train_neg_pids) * 30)\n",
    "test_neg_pids = np.array(list(test_neg_pids) * 30)\n",
    "#seg = pkuseg.pkuseg(model_name='web')\n",
    "#train_dataset = hole_dzdataset(holes, train_pos_pids, train_neg_pids, 16, seg, model)\n",
    "#test_dataset = hole_dzdataset(holes, test_pos_pids, test_neg_pids, 16, seg, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save((train_dataset, test_dataset), 'datasets')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import utils\n",
    "#reload(utils)\n",
    "from utils import split_train, hole_dzdataset, sen_parse\n",
    "from sklearn.model_selection import train_test_split\n",
    "'''\n",
    "train_pos_pids, test_pos_pids = split_train(np.array(pos_pids), 0.2)\n",
    "train_neg_pids, test_neg_pids = split_train(np.array(neg_pids_dup), 0.2)\n",
    "'''\n",
    "seg = pkuseg.pkuseg(model_name='web')\n",
    "a=\" \"\n",
    "fileTrainRead = [a.join(sen_parse(holes[d]['dz_text'], seg))+\"\\n\" for d in train_pos_pids]\n",
    "with open(\"train_pos1117\",'w',encoding='utf-8') as fW:\n",
    "    fW.writelines(fileTrainRead)\n",
    "fileTrainRead = [a.join(sen_parse(holes[d]['dz_text'], seg))+\"\\n\" for d in test_pos_pids]\n",
    "with open(\"test_pos1117\",'w',encoding='utf-8') as fW:\n",
    "    fW.writelines(fileTrainRead)\n",
    "fileTrainRead = [a.join(sen_parse(holes[d]['dz_text'], seg))+\"\\n\" for d in train_neg_pids]\n",
    "with open(\"train_neg1117\",'w',encoding='utf-8') as fW:\n",
    "    fW.writelines(fileTrainRead)\n",
    "fileTrainRead = [a.join(sen_parse(holes[d]['dz_text'], seg))+\"\\n\" for d in test_neg_pids]\n",
    "with open(\"test_neg1117\",'w',encoding='utf-8') as fW:\n",
    "    fW.writelines(fileTrainRead)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "A\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "#reload(utils)\n",
    "from utils import split_train, hole_dzdataset, sen_parse, collate_fn\n",
    "from imp import reload\n",
    "\n",
    "\n",
    "with open(\"train_pos1117\",'r',encoding='utf-8') as fW:\n",
    "    train_pos = [s.split() for s in fW.readlines()]\n",
    "with open(\"test_pos1117\",'r',encoding='utf-8') as fW:\n",
    "    test_pos = [s.split() for s in fW.readlines()]\n",
    "with open(\"train_neg1117\",'r',encoding='utf-8') as fW:\n",
    "    train_neg = [s.split() for s in fW.readlines()]\n",
    "with open(\"test_neg1117\",'r',encoding='utf-8') as fW:\n",
    "    test_neg = [s.split() for s in fW.readlines()]\n",
    "\n",
    "seg = pkuseg.pkuseg(model_name='web')\n",
    "train_dataset = hole_dzdataset(train_pos, train_neg, 16, model)\n",
    "test_dataset = hole_dzdataset(test_pos, test_neg, 16, model)\n",
    "\n",
    "train_dataloader = DataLoader(dataset = train_dataset,\n",
    "                              batch_size = 32,\n",
    "                              shuffle = True,\n",
    "                              num_workers = 0,\n",
    "                              pin_memory = True,\n",
    "                              drop_last = True,\n",
    "                              collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(dataset = test_dataset,\n",
    "                              batch_size = 32,\n",
    "                              shuffle = True,\n",
    "                              num_workers = 0,\n",
    "                              pin_memory = True,\n",
    "                              drop_last = True,\n",
    "                              collate_fn=collate_fn)\n",
    "torch.save((train_dataset,test_dataset,train_dataloader,test_dataloader), \"../data_and_loaders1117\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/czb/anaconda3/envs/torchsparse/lib/python3.7/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 0.8245876236282347 time: 434.05469369888306\n",
      "epoch: 1 0.8310442351984826 time: 453.8799707889557\n",
      "epoch: 2 0.8319672131147541 time: 447.0666227340698\n",
      "epoch: 3 0.8342196179379487 time: 434.70455050468445\n",
      "epoch: 4 0.8363153705459965 time: 443.84785652160645\n",
      "epoch: 5 0.8403205866413765 time: 441.67692732810974\n",
      "epoch: 6 0.8360782753014496 time: 439.6658203601837\n",
      "epoch: 7 0.828499695163257 time: 443.00411915779114\n",
      "epoch: 8 0.834575260804769 time: 440.664085149765\n",
      "epoch: 9 0.8357692047148083 time: 431.9085295200348\n",
      "epoch: 10 0.8360401707085761 time: 444.1946725845337\n",
      "epoch: 11 0.8375389513616042 time: 438.59401202201843\n",
      "epoch: 12 0.8350409836065574 time: 444.54671335220337\n",
      "epoch: 13 0.8360147676466604 time: 430.201105594635\n",
      "epoch: 14 0.8391393442622951 time: 454.4165759086609\n",
      "epoch: 15 0.8417262227340468 time: 444.1811375617981\n",
      "epoch: 16 0.839677042406178 time: 433.80976152420044\n",
      "epoch: 17 0.8374458068012465 time: 426.87681007385254\n",
      "epoch: 18 0.8312008874136296 time: 432.72940468788147\n",
      "epoch: 19 0.833423655331256 time: 429.0554928779602\n",
      "epoch: 20 0.8280212708305108 time: 439.524484872818\n",
      "epoch: 21 0.8298968635686221 time: 445.13366317749023\n",
      "epoch: 22 0.8243208914781195 time: 430.75728964805603\n",
      "epoch: 23 0.8271956713182496 time: 441.6440405845642\n",
      "epoch: 24 0.8225426771440184 time: 446.348153591156\n",
      "epoch: 25 0.8233259382197534 time: 438.1971263885498\n",
      "epoch: 26 0.8214630470125999 time: 446.10983633995056\n",
      "epoch: 27 0.8223521541796505 time: 448.5721664428711\n",
      "epoch: 28 0.8159971548570655 time: 447.0302951335907\n",
      "epoch: 29 0.8136600731608183 time: 442.03663182258606\n",
      "epoch: 30 0.81093347784853 time: 434.3555483818054\n",
      "epoch: 31 0.8095828817233437 time: 450.95219254493713\n",
      "epoch: 32 0.8112002099986452 time: 444.77918553352356\n",
      "epoch: 33 0.8104381181411733 time: 437.19947624206543\n",
      "epoch: 34 0.8114500067741498 time: 443.52807569503784\n",
      "epoch: 35 0.8054125457255115 time: 456.2827904224396\n",
      "epoch: 36 0.8072500338707492 time: 442.67049264907837\n",
      "epoch: 37 0.8049722259856388 time: 430.9621493816376\n",
      "epoch: 38 0.804836742988755 time: 442.61758971214294\n",
      "epoch: 39 0.8061831052702886 time: 445.3223567008972\n",
      "epoch: 40 0.8032956238992006 time: 444.709774017334\n",
      "epoch: 41 0.8022879691098768 time: 426.88800072669983\n",
      "epoch: 42 0.8016740617802466 time: 443.2756016254425\n",
      "epoch: 43 0.8006494716163122 time: 444.714697599411\n",
      "epoch: 44 0.7990787156211896 time: 438.2085108757019\n",
      "epoch: 45 0.7997180260127355 time: 438.8358452320099\n",
      "epoch: 46 0.7971946551957729 time: 435.01810908317566\n",
      "epoch: 47 0.7957720837284921 time: 409.85413789749146\n",
      "epoch: 48 0.7988670234385584 time: 440.527464389801\n",
      "epoch: 49 0.7975375965316353 time: 437.4760072231293\n",
      "epoch: 50 0.7950734995258095 time: 442.60789918899536\n",
      "epoch: 51 0.793714435713318 time: 454.23506712913513\n",
      "epoch: 52 0.7921309781872375 time: 440.05347180366516\n",
      "epoch: 53 0.7936975003387075 time: 441.5208811759949\n",
      "epoch: 54 0.7939134263649912 time: 445.8203663825989\n",
      "epoch: 55 0.7952386194282618 time: 427.9334063529968\n",
      "epoch: 56 0.7948067673756943 time: 440.64595794677734\n",
      "epoch: 57 0.7930920606963826 time: 442.4013500213623\n",
      "epoch: 58 0.7916144492616176 time: 442.0331723690033\n",
      "epoch: 59 0.794374915323127 time: 432.3079068660736\n",
      "epoch: 60 0.7927829901097412 time: 433.4165070056915\n",
      "epoch: 61 0.7920717043761007 time: 431.3327085971832\n",
      "epoch: 62 0.791051348055819 time: 433.7434616088867\n",
      "epoch: 63 0.7918811814117328 time: 438.41118693351746\n",
      "epoch: 64 0.7927448855168676 time: 440.3735795021057\n",
      "epoch: 65 0.7883162850562254 time: 434.25965309143066\n",
      "epoch: 66 0.7903951022896627 time: 439.78405570983887\n",
      "epoch: 67 0.7904882468500203 time: 421.3925395011902\n",
      "epoch: 68 0.7891291830375288 time: 439.165833234787\n",
      "epoch: 69 0.7896711150250644 time: 444.18927931785583\n",
      "epoch: 70 0.7896795827123696 time: 448.8046271800995\n",
      "epoch: 71 0.7900944993903265 time: 442.7666449546814\n",
      "epoch: 72 0.78951869665357 time: 445.69279766082764\n",
      "epoch: 73 0.7889809985096871 time: 448.84259033203125\n",
      "epoch: 74 0.7886422910174773 time: 439.660596370697\n",
      "epoch: 75 0.7884602357404146 time: 444.73427653312683\n",
      "epoch: 76 0.7881342297791627 time: 430.824560880661\n",
      "epoch: 77 0.7872197195501964 time: 441.92186164855957\n",
      "epoch: 78 0.7883374542744885 time: 435.7918155193329\n",
      "epoch: 79 0.7867074244682293 time: 449.89317321777344\n",
      "epoch: 80 0.7882104389649099 time: 452.5451982021332\n",
      "epoch: 81 0.7876346362281533 time: 454.6737947463989\n",
      "epoch: 82 0.7879945129386262 time: 448.4842312335968\n",
      "epoch: 83 0.7871604457390597 time: 444.2684485912323\n",
      "epoch: 84 0.7875245562931852 time: 443.6361300945282\n",
      "epoch: 85 0.7875584270424062 time: 443.284823179245\n",
      "epoch: 86 0.7875711285733641 time: 439.0649588108063\n",
      "epoch: 87 0.7868344397778079 time: 441.30590987205505\n",
      "epoch: 88 0.7875076209185747 time: 444.9664890766144\n",
      "epoch: 89 0.7873425010161225 time: 450.2864480018616\n",
      "epoch: 90 0.7869910919929549 time: 443.88907742500305\n",
      "epoch: 91 0.7860808156076412 time: 439.1835012435913\n",
      "epoch: 92 0.7879987467822788 time: 426.6631371974945\n",
      "epoch: 93 0.7851790069096328 time: 433.8708140850067\n",
      "epoch: 94 0.7878251591925214 time: 446.4811737537384\n",
      "epoch: 95 0.7870207288985233 time: 450.6651623249054\n",
      "epoch: 96 0.785792914239263 time: 430.9189944267273\n",
      "epoch: 97 0.7868852459016393 time: 438.06175899505615\n",
      "epoch: 98 0.7847810256062864 time: 438.7483112812042\n",
      "epoch: 99 0.7875795962606693 time: 441.99726939201355\n"
     ]
    }
   ],
   "source": [
    "from imp import reload\n",
    "import utils\n",
    "reload(utils)\n",
    "import time\n",
    "from utils import lstm_model, test_eval\n",
    "train_dataset,test_dataset,train_dataloader,test_dataloader = torch.load(\"../data_and_loaders1117\")\n",
    "train_dataloader.num_workers = 0\n",
    "test_dataloader.num_workers = 0\n",
    "the_lstm = lstm_model(300, 32)\n",
    "the_lstm = the_lstm.cuda()\n",
    "optimizer = torch.optim.SGD(the_lstm.parameters(), lr = 0.01, momentum = 0.9)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "print(\"start train\")\n",
    "\n",
    "since = time.time()\n",
    "for epoch in range(100):\n",
    "    the_lstm.train()\n",
    "    for i,(x,lens,y) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        y_predict = the_lstm(x, lens)\n",
    "        loss = loss_fn(y_predict, y.cuda())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"epoch:\", epoch, test_eval(test_dataloader, the_lstm), \"time:\", time.time()-since)\n",
    "    since = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reload(utils)\n",
    "#from utils import split_train, hole_dzdataset, sen_parse\n",
    "#ds = hole_dzdataset([['贵校','隔壁','保研'],['信科','保研','百讲']], [['清华','北大'],['中华','人民','共和国']], 2, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/czb/anaconda3/envs/torchsparse/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type lstm_model. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home1/czb/anaconda3/envs/torchsparse/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home1/czb/anaconda3/envs/torchsparse/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sequential. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home1/czb/anaconda3/envs/torchsparse/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home1/czb/anaconda3/envs/torchsparse/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home1/czb/anaconda3/envs/torchsparse/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(the_lstm, 'lstm_1117')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lstm_model(\n",
       "  (rnn): LSTM(300, 300)\n",
       "  (score): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=10, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=10, out_features=2, bias=True)\n",
       "    (3): Softmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
