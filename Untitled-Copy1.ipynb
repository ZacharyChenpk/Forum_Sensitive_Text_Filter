{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pkuseg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../holes_github_dict.pkl','rb') as f:\n",
    "    holes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "857186 10796\n"
     ]
    }
   ],
   "source": [
    "with open('../pids_github.pkl','rb') as f:\n",
    "    pos_pids = pickle.load(f)\n",
    "with open('../deleted_pids.pkl','rb') as f:\n",
    "    neg_pids = pickle.load(f)\n",
    "    \n",
    "pol_pids = list(set(pos_pids) - set(neg_pids))\n",
    "print(len(pos_pids), len(neg_pids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport word2vec\\nimport matplotlib\\nimport matplotlib.pyplot as plt\\nfrom sklearn.decomposition import PCA\\nword2vec.word2vec('corpusSegDone.txt', 'corpusWord2Vec.bin', size=300,verbose=True)\\nmodel = word2vec.load('corpusWord2Vec.bin')\\nrawWordVec=model.vectors\\n\\n# reduce the dimension of word vector\\nX_reduced = PCA(n_components=2).fit_transform(rawWordVec)\\n\\n# show some word(center word) and it's similar words\\nindex1,metrics1 = model.similar(u'中国')\\nindex2,metrics2 = model.similar(u'清华')\\nindex3,metrics3 = model.similar(u'牛顿')\\nindex4,metrics4 = model.similar(u'自动化')\\nindex5,metrics5 = model.similar(u'刘亦菲')\\n\\n# add the index of center word \\nindex01=np.where(model.vocab==u'中国')\\nindex02=np.where(model.vocab==u'清华')\\nindex03=np.where(model.vocab==u'牛顿')\\nindex04=np.where(model.vocab==u'自动化')\\nindex05=np.where(model.vocab==u'刘亦菲')\\n\\nindex1=np.append(index1,index01)\\nindex2=np.append(index2,index03)\\nindex3=np.append(index3,index03)\\nindex4=np.append(index4,index04)\\nindex5=np.append(index5,index05)\\n\\n# plot the result\\n#zhfont = matplotlib.font_manager.FontProperties(fname='/usr/share/fonts/truetype/wqy/wqy-microhei.ttc')\\nfig = plt.figure()\\nax = fig.add_subplot(111)\\n\\nfor i in index1:\\n    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='r')\\n\\nfor i in index2:\\n    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='b')\\n\\nfor i in index3:\\n    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='g')\\n\\nfor i in index4:\\n    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='k')\\n\\nfor i in index5:\\n    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='c')\\n\\nax.axis([0,0.8,-0.5,0.5])\\nplt.show()\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import word2vec\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "word2vec.word2vec('corpusSegDone.txt', 'corpusWord2Vec.bin', size=300,verbose=True)\n",
    "model = word2vec.load('corpusWord2Vec.bin')\n",
    "rawWordVec=model.vectors\n",
    "\n",
    "# reduce the dimension of word vector\n",
    "X_reduced = PCA(n_components=2).fit_transform(rawWordVec)\n",
    "\n",
    "# show some word(center word) and it's similar words\n",
    "index1,metrics1 = model.similar(u'中国')\n",
    "index2,metrics2 = model.similar(u'清华')\n",
    "index3,metrics3 = model.similar(u'牛顿')\n",
    "index4,metrics4 = model.similar(u'自动化')\n",
    "index5,metrics5 = model.similar(u'刘亦菲')\n",
    "\n",
    "# add the index of center word \n",
    "index01=np.where(model.vocab==u'中国')\n",
    "index02=np.where(model.vocab==u'清华')\n",
    "index03=np.where(model.vocab==u'牛顿')\n",
    "index04=np.where(model.vocab==u'自动化')\n",
    "index05=np.where(model.vocab==u'刘亦菲')\n",
    "\n",
    "index1=np.append(index1,index01)\n",
    "index2=np.append(index2,index03)\n",
    "index3=np.append(index3,index03)\n",
    "index4=np.append(index4,index04)\n",
    "index5=np.append(index5,index05)\n",
    "\n",
    "# plot the result\n",
    "#zhfont = matplotlib.font_manager.FontProperties(fname='/usr/share/fonts/truetype/wqy/wqy-microhei.ttc')\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "for i in index1:\n",
    "    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='r')\n",
    "\n",
    "for i in index2:\n",
    "    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='b')\n",
    "\n",
    "for i in index3:\n",
    "    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='g')\n",
    "\n",
    "for i in index4:\n",
    "    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='k')\n",
    "\n",
    "for i in index5:\n",
    "    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='c')\n",
    "\n",
    "ax.axis([0,0.8,-0.5,0.5])\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/czb/anaconda3/envs/torchsparse/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.04781811 -0.04908484 -0.00073652 -0.01611761 -0.08490036 -0.05371191\n",
      " -0.01265323 -0.0452581  -0.08618768 -0.0927384 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import word2vec\n",
    "model = word2vec.load('corpusWord2Vec.bin')\n",
    "a='保研'\n",
    "print(model[a][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from utils import split_train, hole_dzdataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_pos_pids, test_pos_pids = split_train(np.array(pos_pids), 0.2)\n",
    "train_neg_pids, test_neg_pids = split_train(np.array(neg_pids), 0.2)\n",
    "train_neg_pids = np.array(list(train_neg_pids) * 30)\n",
    "test_neg_pids = np.array(list(test_neg_pids) * 30)\n",
    "#seg = pkuseg.pkuseg(model_name='web')\n",
    "#train_dataset = hole_dzdataset(holes, train_pos_pids, train_neg_pids, 16, seg, model)\n",
    "#test_dataset = hole_dzdataset(holes, test_pos_pids, test_neg_pids, 16, seg, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save((train_dataset, test_dataset), 'datasets')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import utils\n",
    "#reload(utils)\n",
    "from utils import split_train, hole_dzdataset, sen_parse\n",
    "from sklearn.model_selection import train_test_split\n",
    "'''\n",
    "train_pos_pids, test_pos_pids = split_train(np.array(pos_pids), 0.2)\n",
    "train_neg_pids, test_neg_pids = split_train(np.array(neg_pids_dup), 0.2)\n",
    "'''\n",
    "seg = pkuseg.pkuseg(model_name='web')\n",
    "a=\" \"\n",
    "fileTrainRead = [a.join(sen_parse(holes[d]['dz_text'], seg))+\"\\n\" for d in train_pos_pids]\n",
    "with open(\"train_pos1117\",'w',encoding='utf-8') as fW:\n",
    "    fW.writelines(fileTrainRead)\n",
    "fileTrainRead = [a.join(sen_parse(holes[d]['dz_text'], seg))+\"\\n\" for d in test_pos_pids]\n",
    "with open(\"test_pos1117\",'w',encoding='utf-8') as fW:\n",
    "    fW.writelines(fileTrainRead)\n",
    "fileTrainRead = [a.join(sen_parse(holes[d]['dz_text'], seg))+\"\\n\" for d in train_neg_pids]\n",
    "with open(\"train_neg1117\",'w',encoding='utf-8') as fW:\n",
    "    fW.writelines(fileTrainRead)\n",
    "fileTrainRead = [a.join(sen_parse(holes[d]['dz_text'], seg))+\"\\n\" for d in test_neg_pids]\n",
    "with open(\"test_neg1117\",'w',encoding='utf-8') as fW:\n",
    "    fW.writelines(fileTrainRead)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "A\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "#reload(utils)\n",
    "from utils import split_train, hole_dzdataset, sen_parse, collate_fn\n",
    "from imp import reload\n",
    "\n",
    "\n",
    "with open(\"train_pos1117\",'r',encoding='utf-8') as fW:\n",
    "    train_pos = [s.split() for s in fW.readlines()]\n",
    "with open(\"test_pos1117\",'r',encoding='utf-8') as fW:\n",
    "    test_pos = [s.split() for s in fW.readlines()]\n",
    "with open(\"train_neg1117\",'r',encoding='utf-8') as fW:\n",
    "    train_neg = [s.split() for s in fW.readlines()]\n",
    "with open(\"test_neg1117\",'r',encoding='utf-8') as fW:\n",
    "    test_neg = [s.split() for s in fW.readlines()]\n",
    "\n",
    "seg = pkuseg.pkuseg(model_name='web')\n",
    "train_dataset = hole_dzdataset(train_pos, train_neg, 16, model)\n",
    "test_dataset = hole_dzdataset(test_pos, test_neg, 16, model)\n",
    "\n",
    "train_dataloader = DataLoader(dataset = train_dataset,\n",
    "                              batch_size = 32,\n",
    "                              shuffle = True,\n",
    "                              num_workers = 0,\n",
    "                              pin_memory = True,\n",
    "                              drop_last = True,\n",
    "                              collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(dataset = test_dataset,\n",
    "                              batch_size = 32,\n",
    "                              shuffle = True,\n",
    "                              num_workers = 0,\n",
    "                              pin_memory = True,\n",
    "                              drop_last = True,\n",
    "                              collate_fn=collate_fn)\n",
    "torch.save((train_dataset,test_dataset,train_dataloader,test_dataloader), \"../data_and_loaders1117\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/czb/anaconda3/envs/torchsparse/lib/python3.7/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 0.8245876236282347 time: 434.05469369888306\n",
      "epoch: 1 0.8310442351984826 time: 453.8799707889557\n",
      "epoch: 2 0.8319672131147541 time: 447.0666227340698\n",
      "epoch: 3 0.8342196179379487 time: 434.70455050468445\n",
      "epoch: 4 0.8363153705459965 time: 443.84785652160645\n",
      "epoch: 5 0.8403205866413765 time: 441.67692732810974\n",
      "epoch: 6 0.8360782753014496 time: 439.6658203601837\n",
      "epoch: 7 0.828499695163257 time: 443.00411915779114\n",
      "epoch: 8 0.834575260804769 time: 440.664085149765\n",
      "epoch: 9 0.8357692047148083 time: 431.9085295200348\n",
      "epoch: 10 0.8360401707085761 time: 444.1946725845337\n",
      "epoch: 11 0.8375389513616042 time: 438.59401202201843\n",
      "epoch: 12 0.8350409836065574 time: 444.54671335220337\n",
      "epoch: 13 0.8360147676466604 time: 430.201105594635\n",
      "epoch: 14 0.8391393442622951 time: 454.4165759086609\n",
      "epoch: 15 0.8417262227340468 time: 444.1811375617981\n",
      "epoch: 16 0.839677042406178 time: 433.80976152420044\n",
      "epoch: 17 0.8374458068012465 time: 426.87681007385254\n",
      "epoch: 18 0.8312008874136296 time: 432.72940468788147\n",
      "epoch: 19 0.833423655331256 time: 429.0554928779602\n",
      "epoch: 20 0.8280212708305108 time: 439.524484872818\n",
      "epoch: 21 0.8298968635686221 time: 445.13366317749023\n"
     ]
    }
   ],
   "source": [
    "from imp import reload\n",
    "import utils\n",
    "reload(utils)\n",
    "import time\n",
    "from utils import lstm_model, test_eval\n",
    "train_dataset,test_dataset,train_dataloader,test_dataloader = torch.load(\"../data_and_loaders1117\")\n",
    "train_dataloader.num_workers = 0\n",
    "test_dataloader.num_workers = 0\n",
    "the_lstm = lstm_model(300, 32)\n",
    "the_lstm = the_lstm.cuda()\n",
    "optimizer = torch.optim.SGD(the_lstm.parameters(), lr = 0.01, momentum = 0.9)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "print(\"start train\")\n",
    "\n",
    "since = time.time()\n",
    "for epoch in range(100):\n",
    "    the_lstm.train()\n",
    "    for i,(x,lens,y) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        y_predict = the_lstm(x, lens)\n",
    "        loss = loss_fn(y_predict, y.cuda())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"epoch:\", epoch, test_eval(test_dataloader, the_lstm), \"time:\", time.time()-since)\n",
    "    since = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reload(utils)\n",
    "#from utils import split_train, hole_dzdataset, sen_parse\n",
    "#ds = hole_dzdataset([['贵校','隔壁','保研'],['信科','保研','百讲']], [['清华','北大'],['中华','人民','共和国']], 2, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(the_lstm, 'lstm_1117')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
