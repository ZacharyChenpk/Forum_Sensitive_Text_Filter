{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pkuseg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../holes_github_dict.pkl','rb') as f:\n",
    "    holes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "857186 10796\n"
     ]
    }
   ],
   "source": [
    "with open('../pids_github.pkl','rb') as f:\n",
    "    pos_pids = pickle.load(f)\n",
    "with open('../deleted_pids.pkl','rb') as f:\n",
    "    neg_pids = pickle.load(f)\n",
    "    \n",
    "pol_pids = list(set(pos_pids) - set(neg_pids))\n",
    "print(len(pos_pids), len(neg_pids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/czb/anaconda3/envs/torchsparse/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file corpusSegDone.txt\n",
      "Vocab size: 76175\n",
      "Words in train file: 16703022\n",
      "Alpha: 0.000005  Progress: 99.99%  Words/thread/sec: 162.06k  24198  Progress: 3.22%  Words/thread/sec: 147.38k  2574  Progress: 9.72%  Words/thread/sec: 154.36k  0k  214  Progress: 11.16%  Words/thread/sec: 155.77k  16.36%  Words/thread/sec: 156.95k  ss: 18.71%  Words/thread/sec: 157.32k  s: 22.70%  Words/thread/sec: 159.01k  159.81k  5.91%  Words/thread/sec: 159.71k  ords/thread/sec: 159.99k  ead/sec: 160.30k  c: 160.55k  rogress: 66.97%  Words/thread/sec: 160.75k  54k  55k   Words/thread/sec: 161.51k  rds/thread/sec: 161.65k  758  Progress: 96.98%  Words/thread/sec: 161.82k  "
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# reduce the dimension of word vector\\nX_reduced = PCA(n_components=2).fit_transform(rawWordVec)\\n\\n# show some word(center word) and it's similar words\\nindex1,metrics1 = model.similar(u'中国')\\nindex2,metrics2 = model.similar(u'清华')\\nindex3,metrics3 = model.similar(u'牛顿')\\nindex4,metrics4 = model.similar(u'自动化')\\nindex5,metrics5 = model.similar(u'刘亦菲')\\n\\n# add the index of center word \\nindex01=np.where(model.vocab==u'中国')\\nindex02=np.where(model.vocab==u'清华')\\nindex03=np.where(model.vocab==u'牛顿')\\nindex04=np.where(model.vocab==u'自动化')\\nindex05=np.where(model.vocab==u'刘亦菲')\\n\\nindex1=np.append(index1,index01)\\nindex2=np.append(index2,index03)\\nindex3=np.append(index3,index03)\\nindex4=np.append(index4,index04)\\nindex5=np.append(index5,index05)\\n\\n# plot the result\\n#zhfont = matplotlib.font_manager.FontProperties(fname='/usr/share/fonts/truetype/wqy/wqy-microhei.ttc')\\nfig = plt.figure()\\nax = fig.add_subplot(111)\\n\\nfor i in index1:\\n    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='r')\\n\\nfor i in index2:\\n    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='b')\\n\\nfor i in index3:\\n    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='g')\\n\\nfor i in index4:\\n    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='k')\\n\\nfor i in index5:\\n    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='c')\\n\\nax.axis([0,0.8,-0.5,0.5])\\nplt.show()\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import word2vec\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "word2vec.word2vec('corpusSegDone.txt', 'corpusWord2Vec.bin', size=300,verbose=True)\n",
    "model = word2vec.load('corpusWord2Vec.bin')\n",
    "rawWordVec=model.vectors\n",
    "\n",
    "# reduce the dimension of word vector\n",
    "X_reduced = PCA(n_components=2).fit_transform(rawWordVec)\n",
    "\n",
    "# show some word(center word) and it's similar words\n",
    "index1,metrics1 = model.similar(u'中国')\n",
    "index2,metrics2 = model.similar(u'清华')\n",
    "index3,metrics3 = model.similar(u'牛顿')\n",
    "index4,metrics4 = model.similar(u'自动化')\n",
    "index5,metrics5 = model.similar(u'刘亦菲')\n",
    "\n",
    "# add the index of center word \n",
    "index01=np.where(model.vocab==u'中国')\n",
    "index02=np.where(model.vocab==u'清华')\n",
    "index03=np.where(model.vocab==u'牛顿')\n",
    "index04=np.where(model.vocab==u'自动化')\n",
    "index05=np.where(model.vocab==u'刘亦菲')\n",
    "\n",
    "index1=np.append(index1,index01)\n",
    "index2=np.append(index2,index03)\n",
    "index3=np.append(index3,index03)\n",
    "index4=np.append(index4,index04)\n",
    "index5=np.append(index5,index05)\n",
    "\n",
    "# plot the result\n",
    "#zhfont = matplotlib.font_manager.FontProperties(fname='/usr/share/fonts/truetype/wqy/wqy-microhei.ttc')\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "for i in index1:\n",
    "    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='r')\n",
    "\n",
    "for i in index2:\n",
    "    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='b')\n",
    "\n",
    "for i in index3:\n",
    "    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='g')\n",
    "\n",
    "for i in index4:\n",
    "    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='k')\n",
    "\n",
    "for i in index5:\n",
    "    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='c')\n",
    "\n",
    "ax.axis([0,0.8,-0.5,0.5])\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.04781811 -0.04908484 -0.00073652 -0.01611761 -0.08490036 -0.05371191\n",
      " -0.01265323 -0.0452581  -0.08618768 -0.0927384 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import word2vec\n",
    "model = word2vec.load('corpusWord2Vec.bin')\n",
    "a='保研'\n",
    "print(model[a][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nneg_pids_dup = neg_pids * 10\\n\\nfrom utils import split_train, hole_dzdataset\\nfrom sklearn.model_selection import train_test_split\\n\\ntrain_pos_pids, test_pos_pids = split_train(np.array(pos_pids), 0.2)\\ntrain_neg_pids, test_neg_pids = split_train(np.array(neg_pids_dup), 0.2)\\nseg = pkuseg.pkuseg(model_name='web')\\ntrain_dataset = hole_dzdataset(holes, train_pos_pids, train_neg_pids, 16, seg, model)\\ntest_dataset = hole_dzdataset(holes, test_pos_pids, test_neg_pids, 16, seg, model)\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "neg_pids_dup = neg_pids * 10\n",
    "\n",
    "from utils import split_train, hole_dzdataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_pos_pids, test_pos_pids = split_train(np.array(pos_pids), 0.2)\n",
    "train_neg_pids, test_neg_pids = split_train(np.array(neg_pids_dup), 0.2)\n",
    "seg = pkuseg.pkuseg(model_name='web')\n",
    "train_dataset = hole_dzdataset(holes, train_pos_pids, train_neg_pids, 16, seg, model)\n",
    "test_dataset = hole_dzdataset(holes, test_pos_pids, test_neg_pids, 16, seg, model)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save((train_dataset, test_dataset), 'datasets')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "neg_pids_dup = neg_pids * 10\n",
    "\n",
    "from utils import split_train, hole_dzdataset, sen_parse\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_pos_pids, test_pos_pids = split_train(np.array(pos_pids), 0.2)\n",
    "train_neg_pids, test_neg_pids = split_train(np.array(neg_pids_dup), 0.2)\n",
    "seg = pkuseg.pkuseg(model_name='web')\n",
    "a=\" \"\n",
    "fileTrainRead = [a.join(sen_parse(holes[d]['dz_text'], seg))+\"\\n\" for d in train_pos_pids]\n",
    "with open(\"train_pos\",'w',encoding='utf-8') as fW:\n",
    "    fW.writelines(fileTrainRead)\n",
    "fileTrainRead = [a.join(sen_parse(holes[d]['dz_text'], seg))+\"\\n\" for d in test_pos_pids]\n",
    "with open(\"test_pos\",'w',encoding='utf-8') as fW:\n",
    "    fW.writelines(fileTrainRead)\n",
    "fileTrainRead = [a.join(sen_parse(holes[d]['dz_text'], seg))+\"\\n\" for d in train_neg_pids]\n",
    "with open(\"train_neg\",'w',encoding='utf-8') as fW:\n",
    "    fW.writelines(fileTrainRead)\n",
    "fileTrainRead = [a.join(sen_parse(holes[d]['dz_text'], seg))+\"\\n\" for d in test_neg_pids]\n",
    "with open(\"test_neg\",'w',encoding='utf-8') as fW:\n",
    "    fW.writelines(fileTrainRead)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "reload(utils)\n",
    "from utils import split_train, hole_dzdataset, sen_parse, collate_fn\n",
    "from imp import reload\n",
    "\n",
    "'''\n",
    "with open(\"train_pos\",'r',encoding='utf-8') as fW:\n",
    "    train_pos = [s.split() for s in fW.readlines()]\n",
    "with open(\"test_pos\",'r',encoding='utf-8') as fW:\n",
    "    test_pos = [s.split() for s in fW.readlines()]\n",
    "with open(\"train_neg\",'r',encoding='utf-8') as fW:\n",
    "    train_neg = [s.split() for s in fW.readlines()]\n",
    "with open(\"test_neg\",'r',encoding='utf-8') as fW:\n",
    "    test_neg = [s.split() for s in fW.readlines()]\n",
    "'''\n",
    "seg = pkuseg.pkuseg(model_name='web')\n",
    "train_dataset = hole_dzdataset(train_pos, train_neg, 16, model)\n",
    "test_dataset = hole_dzdataset(test_pos, test_neg, 16, model)\n",
    "\n",
    "train_dataloader = DataLoader(dataset = train_dataset,\n",
    "                              batch_size = 16,\n",
    "                              shuffle = True,\n",
    "                              num_workers = 4,\n",
    "                              pin_memory = True,\n",
    "                              drop_last = True,\n",
    "                              collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(dataset = test_dataset,\n",
    "                              batch_size = 16,\n",
    "                              shuffle = True,\n",
    "                              num_workers = 4,\n",
    "                              pin_memory = True,\n",
    "                              drop_last = True,\n",
    "                              collate_fn=collate_fn)\n",
    "torch.save((train_dataset,test_dataset,train_dataloader,test_dataloader), \"../data_and_loaders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import lstm_model\n",
    "\n",
    "the_lstm = lstm_model(300, 16)\n",
    "optimizer = torch.optim.SGD(the_lstm.parameters(), lr = 0.01, momentum = 0.9)\n",
    "loss_fn = torch.nn.MSELoss(reduce=False, size_average=False)\n",
    "\n",
    "for epoch in range(500):\n",
    "    for i,(x,lens,y) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        y_predict = the_lstm(x, lens)\n",
    "        loss = loss_fn(y_predict, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n"
     ]
    }
   ],
   "source": [
    "#reload(utils)\n",
    "#from utils import split_train, hole_dzdataset, sen_parse\n",
    "#ds = hole_dzdataset([['贵校','隔壁','保研'],['信科','保研','百讲']], [['清华','北大'],['中华','人民','共和国']], 2, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
