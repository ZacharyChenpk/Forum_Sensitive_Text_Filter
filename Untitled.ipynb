{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pkuseg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../holes_github_dict.pkl','rb') as f:\n",
    "    holes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "857186 10796\n"
     ]
    }
   ],
   "source": [
    "with open('../pids_github.pkl','rb') as f:\n",
    "    pos_pids = pickle.load(f)\n",
    "with open('../deleted_pids.pkl','rb') as f:\n",
    "    neg_pids = pickle.load(f)\n",
    "    \n",
    "pol_pids = list(set(pos_pids) - set(neg_pids))\n",
    "print(len(pos_pids), len(neg_pids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/czb/anaconda3/envs/torchsparse/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file corpusSegDone.txt\n",
      "Vocab size: 76175\n",
      "Words in train file: 16703022\n",
      "Alpha: 0.000005  Progress: 99.99%  Words/thread/sec: 162.06k  24198  Progress: 3.22%  Words/thread/sec: 147.38k  2574  Progress: 9.72%  Words/thread/sec: 154.36k  0k  214  Progress: 11.16%  Words/thread/sec: 155.77k  16.36%  Words/thread/sec: 156.95k  ss: 18.71%  Words/thread/sec: 157.32k  s: 22.70%  Words/thread/sec: 159.01k  159.81k  5.91%  Words/thread/sec: 159.71k  ords/thread/sec: 159.99k  ead/sec: 160.30k  c: 160.55k  rogress: 66.97%  Words/thread/sec: 160.75k  54k  55k   Words/thread/sec: 161.51k  rds/thread/sec: 161.65k  758  Progress: 96.98%  Words/thread/sec: 161.82k  "
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# reduce the dimension of word vector\\nX_reduced = PCA(n_components=2).fit_transform(rawWordVec)\\n\\n# show some word(center word) and it's similar words\\nindex1,metrics1 = model.similar(u'中国')\\nindex2,metrics2 = model.similar(u'清华')\\nindex3,metrics3 = model.similar(u'牛顿')\\nindex4,metrics4 = model.similar(u'自动化')\\nindex5,metrics5 = model.similar(u'刘亦菲')\\n\\n# add the index of center word \\nindex01=np.where(model.vocab==u'中国')\\nindex02=np.where(model.vocab==u'清华')\\nindex03=np.where(model.vocab==u'牛顿')\\nindex04=np.where(model.vocab==u'自动化')\\nindex05=np.where(model.vocab==u'刘亦菲')\\n\\nindex1=np.append(index1,index01)\\nindex2=np.append(index2,index03)\\nindex3=np.append(index3,index03)\\nindex4=np.append(index4,index04)\\nindex5=np.append(index5,index05)\\n\\n# plot the result\\n#zhfont = matplotlib.font_manager.FontProperties(fname='/usr/share/fonts/truetype/wqy/wqy-microhei.ttc')\\nfig = plt.figure()\\nax = fig.add_subplot(111)\\n\\nfor i in index1:\\n    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='r')\\n\\nfor i in index2:\\n    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='b')\\n\\nfor i in index3:\\n    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='g')\\n\\nfor i in index4:\\n    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='k')\\n\\nfor i in index5:\\n    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='c')\\n\\nax.axis([0,0.8,-0.5,0.5])\\nplt.show()\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import word2vec\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "word2vec.word2vec('corpusSegDone.txt', 'corpusWord2Vec.bin', size=300,verbose=True)\n",
    "model = word2vec.load('corpusWord2Vec.bin')\n",
    "rawWordVec=model.vectors\n",
    "\n",
    "# reduce the dimension of word vector\n",
    "X_reduced = PCA(n_components=2).fit_transform(rawWordVec)\n",
    "\n",
    "# show some word(center word) and it's similar words\n",
    "index1,metrics1 = model.similar(u'中国')\n",
    "index2,metrics2 = model.similar(u'清华')\n",
    "index3,metrics3 = model.similar(u'牛顿')\n",
    "index4,metrics4 = model.similar(u'自动化')\n",
    "index5,metrics5 = model.similar(u'刘亦菲')\n",
    "\n",
    "# add the index of center word \n",
    "index01=np.where(model.vocab==u'中国')\n",
    "index02=np.where(model.vocab==u'清华')\n",
    "index03=np.where(model.vocab==u'牛顿')\n",
    "index04=np.where(model.vocab==u'自动化')\n",
    "index05=np.where(model.vocab==u'刘亦菲')\n",
    "\n",
    "index1=np.append(index1,index01)\n",
    "index2=np.append(index2,index03)\n",
    "index3=np.append(index3,index03)\n",
    "index4=np.append(index4,index04)\n",
    "index5=np.append(index5,index05)\n",
    "\n",
    "# plot the result\n",
    "#zhfont = matplotlib.font_manager.FontProperties(fname='/usr/share/fonts/truetype/wqy/wqy-microhei.ttc')\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "for i in index1:\n",
    "    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='r')\n",
    "\n",
    "for i in index2:\n",
    "    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='b')\n",
    "\n",
    "for i in index3:\n",
    "    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='g')\n",
    "\n",
    "for i in index4:\n",
    "    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='k')\n",
    "\n",
    "for i in index5:\n",
    "    ax.text(X_reduced[i][0],X_reduced[i][1], model.vocab[i], fontproperties='SimHei',color='c')\n",
    "\n",
    "ax.axis([0,0.8,-0.5,0.5])\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.04781811 -0.04908484 -0.00073652 -0.01611761 -0.08490036 -0.05371191\n",
      " -0.01265323 -0.0452581  -0.08618768 -0.0927384 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import word2vec\n",
    "model = word2vec.load('corpusWord2Vec.bin')\n",
    "a='保研'\n",
    "print(model[a][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nneg_pids_dup = neg_pids * 10\\n\\nfrom utils import split_train, hole_dzdataset\\nfrom sklearn.model_selection import train_test_split\\n\\ntrain_pos_pids, test_pos_pids = split_train(np.array(pos_pids), 0.2)\\ntrain_neg_pids, test_neg_pids = split_train(np.array(neg_pids_dup), 0.2)\\nseg = pkuseg.pkuseg(model_name='web')\\ntrain_dataset = hole_dzdataset(holes, train_pos_pids, train_neg_pids, 16, seg, model)\\ntest_dataset = hole_dzdataset(holes, test_pos_pids, test_neg_pids, 16, seg, model)\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "neg_pids_dup = neg_pids * 10\n",
    "\n",
    "from utils import split_train, hole_dzdataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_pos_pids, test_pos_pids = split_train(np.array(pos_pids), 0.2)\n",
    "train_neg_pids, test_neg_pids = split_train(np.array(neg_pids_dup), 0.2)\n",
    "seg = pkuseg.pkuseg(model_name='web')\n",
    "train_dataset = hole_dzdataset(holes, train_pos_pids, train_neg_pids, 16, seg, model)\n",
    "test_dataset = hole_dzdataset(holes, test_pos_pids, test_neg_pids, 16, seg, model)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save((train_dataset, test_dataset), 'datasets')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "neg_pids_dup = neg_pids * 10\n",
    "import utils\n",
    "reload(utils)\n",
    "from utils import split_train, hole_dzdataset, sen_parse\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_pos_pids, test_pos_pids = split_train(np.array(pos_pids), 0.2)\n",
    "train_neg_pids, test_neg_pids = split_train(np.array(neg_pids_dup), 0.2)\n",
    "seg = pkuseg.pkuseg(model_name='web')\n",
    "a=\" \"\n",
    "fileTrainRead = [a.join(sen_parse(holes[d]['dz_text'], seg))+\"\\n\" for d in train_pos_pids]\n",
    "with open(\"train_pos\",'w',encoding='utf-8') as fW:\n",
    "    fW.writelines(fileTrainRead)\n",
    "fileTrainRead = [a.join(sen_parse(holes[d]['dz_text'], seg))+\"\\n\" for d in test_pos_pids]\n",
    "with open(\"test_pos\",'w',encoding='utf-8') as fW:\n",
    "    fW.writelines(fileTrainRead)\n",
    "fileTrainRead = [a.join(sen_parse(holes[d]['dz_text'], seg))+\"\\n\" for d in train_neg_pids]\n",
    "with open(\"train_neg\",'w',encoding='utf-8') as fW:\n",
    "    fW.writelines(fileTrainRead)\n",
    "fileTrainRead = [a.join(sen_parse(holes[d]['dz_text'], seg))+\"\\n\" for d in test_neg_pids]\n",
    "with open(\"test_neg\",'w',encoding='utf-8') as fW:\n",
    "    fW.writelines(fileTrainRead)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "A\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "reload(utils)\n",
    "from utils import split_train, hole_dzdataset, sen_parse, collate_fn\n",
    "from imp import reload\n",
    "\n",
    "\n",
    "with open(\"train_pos\",'r',encoding='utf-8') as fW:\n",
    "    train_pos = [s.split() for s in fW.readlines()]\n",
    "with open(\"test_pos\",'r',encoding='utf-8') as fW:\n",
    "    test_pos = [s.split() for s in fW.readlines()]\n",
    "with open(\"train_neg\",'r',encoding='utf-8') as fW:\n",
    "    train_neg = [s.split() for s in fW.readlines()]\n",
    "with open(\"test_neg\",'r',encoding='utf-8') as fW:\n",
    "    test_neg = [s.split() for s in fW.readlines()]\n",
    "\n",
    "seg = pkuseg.pkuseg(model_name='web')\n",
    "train_dataset = hole_dzdataset(train_pos, train_neg, 16, model)\n",
    "test_dataset = hole_dzdataset(test_pos, test_neg, 16, model)\n",
    "\n",
    "train_dataloader = DataLoader(dataset = train_dataset,\n",
    "                              batch_size = 16,\n",
    "                              shuffle = True,\n",
    "                              num_workers = 4,\n",
    "                              pin_memory = True,\n",
    "                              drop_last = True,\n",
    "                              collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(dataset = test_dataset,\n",
    "                              batch_size = 16,\n",
    "                              shuffle = True,\n",
    "                              num_workers = 4,\n",
    "                              pin_memory = True,\n",
    "                              drop_last = True,\n",
    "                              collate_fn=collate_fn)\n",
    "torch.save((train_dataset,test_dataset,train_dataloader,test_dataloader), \"../data_and_loaders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start train\n",
      "epoch: 0 0.9039963942307693 time: 520.5856945514679\n",
      "epoch: 1 0.9070737317639257 time: 492.98219299316406\n",
      "epoch: 2 0.9076436090848806 time: 494.2607092857361\n",
      "epoch: 3 0.9089025198938993 time: 491.58807706832886\n",
      "epoch: 4 0.908840351458886 time: 496.4952869415283\n",
      "epoch: 5 0.9083896303050398 time: 495.4669532775879\n",
      "epoch: 6 0.910746850132626 time: 490.3245837688446\n",
      "epoch: 7 0.9109696203580901 time: 506.41914892196655\n",
      "epoch: 8 0.9100163710212201 time: 494.80792474746704\n",
      "epoch: 9 0.90920300066313 time: 492.5889320373535\n",
      "epoch: 10 0.9143008123342176 time: 501.9008071422577\n",
      "epoch: 11 0.9143422579575596 time: 493.60408210754395\n",
      "epoch: 12 0.9084725215517241 time: 494.1197991371155\n",
      "epoch: 13 0.9156115301724138 time: 495.11815786361694\n",
      "epoch: 14 0.9142127403846154 time: 495.6083562374115\n",
      "epoch: 15 0.9157773126657824 time: 504.98693013191223\n",
      "epoch: 16 0.9145650281830239 time: 495.20422649383545\n",
      "epoch: 17 0.9163471899867374 time: 491.61608719825745\n",
      "epoch: 18 0.9188805537135278 time: 502.4019603729248\n",
      "epoch: 19 0.9193105520557029 time: 493.25163531303406\n",
      "epoch: 20 0.9201135610079576 time: 492.0724308490753\n",
      "epoch: 21 0.9201653680371353 time: 499.1234323978424\n",
      "epoch: 22 0.9202586206896551 time: 495.04952025413513\n",
      "epoch: 23 0.9204347645888594 time: 493.4907703399658\n",
      "epoch: 24 0.9224966843501327 time: 501.79414415359497\n",
      "epoch: 25 0.9220666860079576 time: 492.46727323532104\n",
      "epoch: 26 0.9223309018567639 time: 503.61249899864197\n",
      "epoch: 27 0.9234084880636605 time: 495.42502212524414\n",
      "epoch: 28 0.924895349801061 time: 494.7117667198181\n",
      "epoch: 29 0.9258900447612732 time: 504.38460969924927\n",
      "epoch: 30 0.9269261853448276 time: 490.67395186424255\n",
      "epoch: 31 0.9261853448275862 time: 490.4900710582733\n",
      "epoch: 32 0.9291072612732095 time: 492.00023770332336\n",
      "epoch: 33 0.929620150862069 time: 491.6860635280609\n",
      "epoch: 34 0.9325161637931034 time: 492.2290632724762\n",
      "epoch: 35 0.9335315815649867 time: 491.19139552116394\n",
      "epoch: 36 0.9292471402519894 time: 491.72365641593933\n",
      "epoch: 37 0.9350443468169761 time: 495.5855724811554\n",
      "epoch: 38 0.9338838693633952 time: 494.9828357696533\n",
      "epoch: 39 0.9389091511936339 time: 492.6358332633972\n",
      "epoch: 40 0.9351997679045093 time: 492.91374611854553\n",
      "epoch: 41 0.9421833554376657 time: 489.95621252059937\n",
      "epoch: 42 0.9428464854111406 time: 493.79790925979614\n",
      "epoch: 43 0.9443281664456233 time: 492.5072069168091\n",
      "epoch: 44 0.9456958720159151 time: 493.0438964366913\n",
      "epoch: 45 0.9496539290450928 time: 493.351279258728\n",
      "epoch: 46 0.9466594827586207 time: 490.7735333442688\n",
      "epoch: 47 0.9507056117374005 time: 491.5254409313202\n",
      "epoch: 48 0.9504983836206896 time: 494.6311538219452\n",
      "epoch: 49 0.9519178962201591 time: 493.4129297733307\n",
      "epoch: 50 0.9153939406498673 time: 491.20488023757935\n",
      "epoch: 51 0.9435355188992043 time: 494.6754903793335\n",
      "epoch: 52 0.9479235742705571 time: 491.46409344673157\n",
      "epoch: 53 0.9528866876657824 time: 501.282249212265\n",
      "epoch: 54 0.9530472894562334 time: 497.91064405441284\n",
      "epoch: 55 0.9553993285809018 time: 503.5518090724945\n",
      "epoch: 56 0.9534824685013262 time: 505.39708828926086\n",
      "epoch: 57 0.9552283653846154 time: 500.37536358833313\n",
      "epoch: 58 0.9569224552387268 time: 500.0077714920044\n",
      "epoch: 59 0.9584248590848806 time: 504.87406301498413\n",
      "epoch: 60 0.9601603945623343 time: 493.48286652565\n",
      "epoch: 61 0.9594454575596817 time: 501.56131625175476\n",
      "epoch: 62 0.9580725712864722 time: 508.0958433151245\n",
      "epoch: 63 0.9578601624668435 time: 505.7980525493622\n",
      "epoch: 64 0.960346899867374 time: 512.1056814193726\n",
      "epoch: 65 0.9604453332228117 time: 504.28079056739807\n",
      "epoch: 66 0.9597200348143236 time: 490.23025727272034\n",
      "epoch: 67 0.9601396717506632 time: 485.9716143608093\n",
      "epoch: 68 0.9611084631962865 time: 488.117582321167\n",
      "epoch: 69 0.9603520805702918 time: 487.81907749176025\n",
      "epoch: 70 0.9622793020557029 time: 490.47109270095825\n",
      "epoch: 71 0.9601137682360743 time: 483.8638331890106\n",
      "epoch: 72 0.962253398541114 time: 489.7888379096985\n",
      "epoch: 73 0.9623984582228117 time: 485.35369658470154\n",
      "epoch: 74 0.9618544844164456 time: 487.15311098098755\n",
      "epoch: 75 0.9638024287135278 time: 484.34898948669434\n",
      "epoch: 76 0.9626056863395226 time: 485.1056218147278\n",
      "epoch: 77 0.9621446037798409 time: 484.26999139785767\n",
      "epoch: 78 0.9628336372679045 time: 489.6720676422119\n",
      "epoch: 79 0.9627559267241379 time: 490.58010601997375\n",
      "epoch: 80 0.9624761687665783 time: 486.75601744651794\n",
      "epoch: 81 0.9622585792440318 time: 491.5339334011078\n",
      "epoch: 82 0.9621342423740054 time: 489.1367826461792\n",
      "epoch: 83 0.9637558023872679 time: 490.8164713382721\n",
      "epoch: 84 0.9620150862068966 time: 488.13503646850586\n",
      "epoch: 85 0.9631133952254642 time: 489.81779050827026\n",
      "epoch: 86 0.9617612317639257 time: 488.9844846725464\n",
      "epoch: 87 0.9633154426392573 time: 488.6844127178192\n",
      "epoch: 88 0.9638645971485411 time: 493.59363436698914\n",
      "epoch: 89 0.9639164041777188 time: 490.5197184085846\n",
      "epoch: 90 0.9623259283819628 time: 490.42121982574463\n",
      "epoch: 91 0.964253149867374 time: 489.60773038864136\n",
      "epoch: 92 0.9641236322944297 time: 488.45850682258606\n",
      "epoch: 93 0.9626989389920424 time: 493.71665930747986\n",
      "epoch: 94 0.9635848391909815 time: 492.6818325519562\n",
      "epoch: 95 0.9635537549734748 time: 493.8318588733673\n",
      "epoch: 96 0.9635226707559682 time: 492.82059240341187\n",
      "epoch: 97 0.9623103862732095 time: 492.92100191116333\n",
      "epoch: 98 0.9636262848143236 time: 494.3678500652313\n",
      "epoch: 99 0.9630305039787799 time: 496.105623960495\n"
     ]
    }
   ],
   "source": [
    "from imp import reload\n",
    "import utils\n",
    "reload(utils)\n",
    "import time\n",
    "from utils import lstm_model, test_eval\n",
    "train_dataset,test_dataset,train_dataloader,test_dataloader = torch.load(\"../data_and_loaders\")\n",
    "train_dataloader.num_workers = 0\n",
    "test_dataloader.num_workers = 0\n",
    "the_lstm = lstm_model(300, 16)\n",
    "the_lstm = the_lstm.cuda()\n",
    "optimizer = torch.optim.SGD(the_lstm.parameters(), lr = 0.01, momentum = 0.9)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "print(\"start train\")\n",
    "\n",
    "since = time.time()\n",
    "for epoch in range(100):\n",
    "    the_lstm.train()\n",
    "    for i,(x,lens,y) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        y_predict = the_lstm(x, lens)\n",
    "        loss = loss_fn(y_predict, y.cuda())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"epoch:\", epoch, test_eval(test_dataloader, the_lstm), \"time:\", time.time()-since)\n",
    "    since = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reload(utils)\n",
    "#from utils import split_train, hole_dzdataset, sen_parse\n",
    "#ds = hole_dzdataset([['贵校','隔壁','保研'],['信科','保研','百讲']], [['清华','北大'],['中华','人民','共和国']], 2, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(the_lstm, 'lstm_1112')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lstm_model(\n",
       "  (rnn): LSTM(300, 300)\n",
       "  (score): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=10, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=10, out_features=2, bias=True)\n",
       "    (3): Softmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
